---
title: 'Deployment of dynamic quantization deepseek-r1'
description: 'Deployment of dynamic quantization deepseek-r1'
date: '2025/02/01'
image: '/blogs-img/dynamic-quantized-ds-r1.png'
published: true
tags: ['quantization', 'LLM', 'deepseek-r1']
ontop: true
---

最近[DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1)热度不减，由于其API处于不可用状态，故尝试私有化部署模型，本文采用1.73-bit动态量化部署。

> 本文主要参考[snowkylin](https://snowkylin.github.io/blogs/a-note-on-deepseek-r1.html)和[unsloth](https://unsloth.ai/blog/deepseekr1-dynamic)的工作。  
> 感谢ollama, llama.cpp, deepseek等开源产品提供支持💕。感谢[autodl-private](https://private.autodl.com/)与[cloudpods](https://www.cloudpods.org/)提供私有云平台技术支持✨。

## 1. 准备工作

1. 下载[unsloth](https://unsloth.ai/blog/deepseekr1-dynamic)量化的1.73-bit模型, 该模型使用重要性矩阵(importance matrix)来校准量化过程，以允许较低位的表示。
   ![dynamic-quantized-ds-r1](/blogs-img/dynamic-quantized-ds-r1.png)

2. 设备要求，本文使用两张RTX4090(24GB VRAM), 32vcore(Gold 6133), 238GB RAM。Dokcer容器环境

## 2. ollama安装

[ollama](https://ollama.ai/download)是一个本地的AI模型服务，支持多种模型，包括LLM、文本生成、图像生成等。

### 手动安装ollama (Manually)

Reference: [Manual install](https://github.com/ollama/ollama/blob/main/docs/linux.md#manual-install)

```sh
curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz
# The download link can be replaced with the github mirror links.
sudo tar -C /usr -xzf ollama-linux-amd64.tgz
```

### 启动ollama

本文由于ollama安装在docker容器内，无法使用`systemctl`，因此使用nohup手动后台运行ollama
创建ollama启动脚本

```sh [/etc/ollama-start.sh]
#!/bin/bash
/usr/bin/ollama serve
```

启动ollama

```sh
nohup /etc/ollama-start.sh > ollama-service.log 2>&1 &
```

## 3.安装llama.cpp

[llama.cpp](https://github.com/ggerganov/llama.cpp)是一个C++库，用于使用LLaMA模型进行推理。

```sh
# 手动下载llama.cpp预构建版本(pre-built versions)
curl -L https://github.com/ggerganov/llama.cpp/releases/download/b4610/llama-b4610-bin-ubuntu-x64.zip -o llama-b4610-bin-ubuntu-x64.zip
unzip llama-b4610-bin-ubuntu-x64.zip
cd build/bin
```

## 4. 合并模型

Use**llama-gguf-split** suggested [here](https://unsloth.ai/blog/deepseekr1-dynamic)

```sh
./llama-gguf-split --merge DeepSeek-R1-UD-IQ1_M-00001-of-00004.gguf DeepSeek-R1-UD-IQ1_S.gguf
```

To change model directory, use `OLLAMA_MODELS` environment variable.

## 5. 创建ollama模型

创建`modelfile`, 该文件旨在描述模型信息，便于ollama创建模型。

```sh [DeepSeekQ1_Modelfile]
 FROM /root/nfs_public/models/quantization/DeepSeek-R1-UD-IQ1_S.gguf
 PARAMETER num_gpu 14
 PARAMETER num_ctx 2048
 PARAMETER temperature 0.6
 TEMPLATE "<｜User｜>{{ .Prompt }}<｜Assistant｜>"
```

`num_gpu`参数是卸载到gpu的模型层数，据参考该模型有61层， 可以将7层卸载到每个RTX 4090 GPU(24GB VRAM)。我这里将14层卸载到两个RTX 4090 GPU(24GB VRAM)。

```sh
ollama create DeepSeek-R1-UD-IQ1_S -f DeepSeekQ1_Modelfile
```

## 6. 模型部署

```sh
ollama run DeepSeek-R1-UD-IQ1_S --verbose
```
