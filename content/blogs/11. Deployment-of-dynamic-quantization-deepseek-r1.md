---
title: 'Deployment of dynamic quantization deepseek-r1'
description: 'Deployment of dynamic quantization deepseek-r1'
date: '2025/02/01'
image: '/blogs-img/dynamic-quantized-ds-r1.png'
published: true
tags: ['quantization', 'LLM', 'deepseek-r1']
ontop: true
---

æœ€è¿‘[DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1)çƒ­åº¦ä¸å‡ï¼Œç”±äºå…¶APIå¤„äºä¸å¯ç”¨çŠ¶æ€ï¼Œæ•…å°è¯•ç§æœ‰åŒ–éƒ¨ç½²æ¨¡å‹ï¼Œæœ¬æ–‡é‡‡ç”¨1.73-bitåŠ¨æ€é‡åŒ–éƒ¨ç½²ã€‚

> æœ¬æ–‡ä¸»è¦å‚è€ƒ[snowkylin](https://snowkylin.github.io/blogs/a-note-on-deepseek-r1.html)å’Œ[unsloth](https://unsloth.ai/blog/deepseekr1-dynamic)çš„å·¥ä½œã€‚  
> æ„Ÿè°¢ollama, llama.cpp, deepseekç­‰å¼€æºäº§å“æä¾›æ”¯æŒğŸ’•ã€‚æ„Ÿè°¢[autodl-private](https://private.autodl.com/)ä¸[cloudpods](https://www.cloudpods.org/)æä¾›ç§æœ‰äº‘å¹³å°æŠ€æœ¯æ”¯æŒâœ¨ã€‚

## 1. å‡†å¤‡å·¥ä½œ

1. ä¸‹è½½[unsloth](https://unsloth.ai/blog/deepseekr1-dynamic)é‡åŒ–çš„1.73-bitæ¨¡å‹, è¯¥æ¨¡å‹ä½¿ç”¨é‡è¦æ€§çŸ©é˜µ(importance matrix)æ¥æ ¡å‡†é‡åŒ–è¿‡ç¨‹ï¼Œä»¥å…è®¸è¾ƒä½ä½çš„è¡¨ç¤ºã€‚
   ![dynamic-quantized-ds-r1](/blogs-img/dynamic-quantized-ds-r1.png)

2. è®¾å¤‡è¦æ±‚ï¼Œæœ¬æ–‡ä½¿ç”¨ä¸¤å¼ RTX4090(24GB VRAM), 32vcore(Gold 6133), 238GB RAMã€‚Dokcerå®¹å™¨ç¯å¢ƒ

## 2. ollamaå®‰è£…

[ollama](https://ollama.ai/download)æ˜¯ä¸€ä¸ªæœ¬åœ°çš„AIæ¨¡å‹æœåŠ¡ï¼Œæ”¯æŒå¤šç§æ¨¡å‹ï¼ŒåŒ…æ‹¬LLMã€æ–‡æœ¬ç”Ÿæˆã€å›¾åƒç”Ÿæˆç­‰ã€‚

### æ‰‹åŠ¨å®‰è£…ollama (Manually)

Reference: [Manual install](https://github.com/ollama/ollama/blob/main/docs/linux.md#manual-install)

```sh
curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz
# The download link can be replaced with the github mirror links.
sudo tar -C /usr -xzf ollama-linux-amd64.tgz
```

### å¯åŠ¨ollama

æœ¬æ–‡ç”±äºollamaå®‰è£…åœ¨dockerå®¹å™¨å†…ï¼Œæ— æ³•ä½¿ç”¨`systemctl`ï¼Œå› æ­¤ä½¿ç”¨nohupæ‰‹åŠ¨åå°è¿è¡Œollama
åˆ›å»ºollamaå¯åŠ¨è„šæœ¬

```sh [/etc/ollama-start.sh]
#!/bin/bash
/usr/bin/ollama serve
```

å¯åŠ¨ollama

```sh
nohup /etc/ollama-start.sh > ollama-service.log 2>&1 &
```

## 3.å®‰è£…llama.cpp

[llama.cpp](https://github.com/ggerganov/llama.cpp)æ˜¯ä¸€ä¸ªC++åº“ï¼Œç”¨äºä½¿ç”¨LLaMAæ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

```sh
# æ‰‹åŠ¨ä¸‹è½½llama.cppé¢„æ„å»ºç‰ˆæœ¬(pre-built versions)
curl -L https://github.com/ggerganov/llama.cpp/releases/download/b4610/llama-b4610-bin-ubuntu-x64.zip -o llama-b4610-bin-ubuntu-x64.zip
unzip llama-b4610-bin-ubuntu-x64.zip
cd build/bin
```

## 4. åˆå¹¶æ¨¡å‹

Use**llama-gguf-split** suggested [here](https://unsloth.ai/blog/deepseekr1-dynamic)

```sh
./llama-gguf-split --merge DeepSeek-R1-UD-IQ1_M-00001-of-00004.gguf DeepSeek-R1-UD-IQ1_S.gguf
```

To change model directory, use `OLLAMA_MODELS` environment variable.

## 5. åˆ›å»ºollamaæ¨¡å‹

åˆ›å»º`modelfile`, è¯¥æ–‡ä»¶æ—¨åœ¨æè¿°æ¨¡å‹ä¿¡æ¯ï¼Œä¾¿äºollamaåˆ›å»ºæ¨¡å‹ã€‚

```sh [DeepSeekQ1_Modelfile]
 FROM /root/nfs_public/models/quantization/DeepSeek-R1-UD-IQ1_S.gguf
 PARAMETER num_gpu 14
 PARAMETER num_ctx 2048
 PARAMETER temperature 0.6
 TEMPLATE "<ï½œUserï½œ>{{ .Prompt }}<ï½œAssistantï½œ>"
```

`num_gpu`å‚æ•°æ˜¯å¸è½½åˆ°gpuçš„æ¨¡å‹å±‚æ•°ï¼Œæ®å‚è€ƒè¯¥æ¨¡å‹æœ‰61å±‚ï¼Œ å¯ä»¥å°†7å±‚å¸è½½åˆ°æ¯ä¸ªRTX 4090 GPU(24GB VRAM)ã€‚æˆ‘è¿™é‡Œå°†14å±‚å¸è½½åˆ°ä¸¤ä¸ªRTX 4090 GPU(24GB VRAM)ã€‚

```sh
ollama create DeepSeek-R1-UD-IQ1_S -f DeepSeekQ1_Modelfile
```

## 6. æ¨¡å‹éƒ¨ç½²

```sh
ollama run DeepSeek-R1-UD-IQ1_S --verbose
```
